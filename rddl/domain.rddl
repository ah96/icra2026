// ICRA 2026 RDDL domain: situationâ†’response with explanation shaping and delivery
// -----------------------------------------------------------------------------
// How to run prost
// ./run-server.py -b ./icra2026/
// ./prost.py icra2026_instance "[Prost -s 1 -se [IPC2014]]"
// -----------------------------------------------------------------------------

domain icra2026 {

    // ===== Types =====    
    types {
        human: object;
        robot: object;
        book: object;
        waypoint: object;
        situation  : object;       // normal_success, agent_error, uncertainty, inability, suboptimal, norm_violation, unforeseen
        response_type : object;    // apology, why_expl, what_expl, next_action, ask_for_help, continue
    };


    // ===== Parameters & Variables =====
    pvariables {

        // ---------- Non-Fluents ----------
        // ---------- World topology ----------
        connected(waypoint, waypoint): { non-fluent, bool, default = false };  // undirected edges should be listed both ways in the instance    

        // ---------- Task state (navigation & manipulation) ----------
        human_at(human, waypoint): { non-fluent, bool, default = false };
        human_wants_book(human, book): { non-fluent, bool, default = false };

        // ---------- Situation --> Response core ----------
        // One-hot flag indicating the currently active situation for a (robot,human)
        is_active_situation(robot, human, situation): { non-fluent, bool, default = false };

        // Prior over response types for each situation (from user study)
        p_resp(response_type, situation): { non-fluent, real, default = 0.5 };

        // Helper: which response types are "explanations" (set in instance; e.g., why_expl/what_expl = true)
        is_explanation_type(response_type): { non-fluent, bool, default = false };

        // Helper: which situation allows fetching a book
        situation_allows_fetching(situation): { non-fluent, bool, default = false };

        // Attribute families: modality, scope, detail, duration, tone, justification
        // Per-human baselines (probabilities of the FIRST value in each family; complement is the other value)
        visual_modality_prob(human): { non-fluent, real, default = 0.5 };   // textual = 1 - this
        local_scope_prob(human): { non-fluent, real, default = 0.5 };       // global = 1 - this
        summary_detail_prob(human): { non-fluent, real, default = 0.5 };    // narrative = 1 - this
        short_duration_prob(human): { non-fluent, real, default = 0.5 };    // long = 1 - this
        justification_contrastive_prob(human): { non-fluent, real, default = 0.5 }; // standard = 1 - this

        // Normative priors conditioned on response type (probability of FIRST value in family)
        p_visual_given_rt(response_type): { non-fluent, real, default = 0.5 };
        p_local_scope_given_rt(response_type): { non-fluent, real, default = 0.5 };
        p_summary_detail_given_rt(response_type): { non-fluent, real, default = 0.5 };
        p_short_duration_given_rt(response_type): { non-fluent, real, default = 0.5 };
        p_just_contrastive_given_rt(response_type): { non-fluent, real, default = 0.5 };

        // Mixing weights between baselines and response-type norms
        alpha_modality: { non-fluent, real, default = 0.5 };
        alpha_scope: { non-fluent, real, default = 0.5 };
        alpha_detail: { non-fluent, real, default = 0.5 };
        alpha_duration: { non-fluent, real, default = 0.5 };
        alpha_justification: { non-fluent, real, default = 0.5 };



        // ---------- State-Fluents ----------
        // ---------- Task state (navigation & manipulation) ----------
        robot_at(robot, waypoint): { state-fluent, bool, default = false };
        book_at(book, waypoint): { state-fluent, bool, default = false };
        holding(robot, book): { state-fluent, bool, default = false };
        not_holding(robot, book): { state-fluent, bool, default = false };
        delivered(book, human): { state-fluent, bool, default = false }; // whether book has been delivered to human
        asked_for_book(?h, ?r, ?bk): { state-fluent, bool, default = false };

        // Which response type is selected for a (robot,human,situation); persists until changed
        selected_response(response_type, robot, human, situation): { state-fluent, bool, default = false };

        // ---------- Explanation planning pipeline ----------
        explanation_planning_started(response_type, robot, human): { state-fluent, bool, default = false };
        explanation_planning_ended(response_type, robot, human): { state-fluent, bool, default = false };

        // Wanted samples per human (stochastic preferences drawn each step)
        modality_visual_wanted(human): { state-fluent, bool, default = false }; // textual_wanted = !visual_wanted
        scope_local_wanted(human): { state-fluent, bool, default = false };     // global_wanted = !local_wanted
        detail_summary_wanted(human): { state-fluent, bool, default = false };  // narrative_wanted = !summary_wanted
        duration_short_wantagent_errored(human): { state-fluent, bool, default = false };  // long_wanted = !short_wanted
        justification_contrastive_wanted(human): { state-fluent, bool, default = false }; // standard_wanted = !contrastive_wanted

        // Chosen attribute values (persist for a (robot,human,book))
        modality_visual_selected(robot, human, book): { state-fluent, bool, default = false };
        modality_textual_selected(robot, human, book): { state-fluent, bool, default = false };

        scope_local_selected(robot, human, book): { state-fluent, bool, default = false };
        scope_global_selected(robot, human, book): { state-fluent, bool, default = false };

        detail_summary_selected(robot, human, book): { state-fluent, bool, default = false };
        detail_narrative_selected(robot, human, book): { state-fluent, bool, default = false };

        duration_short_selected(robot, human, book): { state-fluent, bool, default = false };
        duration_long_selected(robot, human, book): { state-fluent, bool, default = false };

        justification_standard_selected(robot, human, book): { state-fluent, bool, default = false };
        justification_contrastive_selected(robot, human, book): { state-fluent, bool, default = false };


        // ---------- Action-Fluents ----------
        // Navigation & manipulation
        goto_waypoint(robot, waypoint, waypoint): { action-fluent, bool, default = false };
        fetch_book(robot, book): { action-fluent, bool, default = false };
        give_book(robot, book, human): { action-fluent, bool, default = false };
        ask_for_book(human, robot, book): { action-fluent, bool, default = false };

        // Action to respond with a given type in the current situation (one per step)
        respond(response_type, robot, human, situation): { action-fluent, bool, default = false };

        // Explanation planning
        start_explanation_planning(response_type, robot, human, situation): { action-fluent, bool, default = false };
        end_explanation_planning(response_type, robot, human, situation): { action-fluent, bool, default = false };

        // Attribute selections
        choose_visual_modality(response_type, robot, human, situation): { action-fluent, bool, default = false };
        choose_textual_modality(response_type, robot, human, situation): { action-fluent, bool, default = false };

        choose_local_scope(response_type, robot, human, situation): { action-fluent, bool, default = false };
        choose_global_scope(response_type, robot, human, situation): { action-fluent, bool, default = false };

        choose_summary_detail(response_type, robot, human, situation): { action-fluent, bool, default = false };
        choose_narrative_detail(response_type, robot, human, situation): { action-fluent, bool, default = false };

        choose_short_duration(response_type, robot, human, situation): { action-fluent, bool, default = false };
        choose_long_duration(response_type, robot, human, situation): { action-fluent, bool, default = false };

        choose_justification_standard(response_type, robot, human, situation): { action-fluent, bool, default = false };
        choose_justification_contrastive(response_type, robot, human, situation): { action-fluent, bool, default = false };
    
    };


    // ===== Dynamics =====
    cpfs {

        // --- Navigation dynamics (single-step move)
        robot_at'(?r, ?w) = if (exists_{?w1: waypoint} (goto_waypoint(?r, ?w1, ?w))) then true 
                            else if (exists_{?w1: waypoint} (goto_waypoint(?r, ?w, ?w1))) then false
                            else robot_at(?r, ?w);

        // --- Book movement
        book_at'(?bk, ?w) =
            ( book_at(?bk, ?w) & ~exists_{?r: robot} [ fetch_book(?r, ?bk) & robot_at(?r, ?w) ] )
            | exists_{?r: robot, ?h: human} [ give_book(?r, ?bk, ?h) & robot_at(?r, ?w) & human_at(?h, ?w) ];

        // --- Holding dynamics ---
        holding'(?r, ?bk) = holding(?r, ?bk) | fetch_book(?r, ?bk);
        not_holding'(?r, ?bk) = not_holding(?r, ?bk) | exists_{?h: human} [ give_book(?r, ?bk, ?h) ];

        // --- Book delivered
        delivered'(?bk, ?h) = delivered(?bk, ?h) | exists_{?r: robot} [ give_book(?r, ?bk, ?h) ];
        
        // --- Human asked for a book?
        asked_for_book'(?h, ?r, ?bk) = ask_for_book(?h, ?r, ?bk) | asked_for_book(?h, ?r, ?bk);

        // --- Response selection persistence/update
        selected_response'(?rt, ?r, ?h, ?s) = respond(?rt, ?r, ?h, ?s) | selected_response(?rt, ?r, ?h, ?s);

        // --- Explanation pipeline persistence
        explanation_planning_started'(?rt, ?r, ?h, ?s) = explanation_planning_started(?rt, ?r, ?h, ?s) | start_explanation_planning(?rt, ?r, ?h, ?s);
        explanation_planning_ended'(?rt, ?r, ?h, ?s) = explanation_planning_ended(?rt, ?r, ?h, ?s) | end_explanation_planning(?rt, ?r, ?h, ?s);

        // --- Wanted preferences (mixture of per-human baselines and response-type norms, weighted by situation) ---
        modality_visual_wanted'(?h) = Bernoulli(
            (1 - alpha_modality) * visual_modality_prob(?h)
          + alpha_modality * ( sum_{?rb: robot, ?s: situation} [
                if ( is_active_situation(?rb, ?h, ?s) ) then ( sum_{?rt: response_type} [ p_resp(?rt, ?s) * p_visual_given_rt(?rt) ] ) else 0
            ] )
        );
        scope_local_wanted'(?h) = Bernoulli(
            (1 - alpha_scope) * local_scope_prob(?h)
          + alpha_scope * ( sum_{?rb: robot, ?s: situation} [
                if ( is_active_situation(?rb, ?h, ?s) ) then ( sum_{?rt: response_type} [ p_resp(?rt, ?s) * p_local_scope_given_rt(?rt) ] ) else 0
            ] )
        );
        detail_summary_wanted'(?h) = Bernoulli(
            (1 - alpha_detail) * summary_detail_prob(?h)
          + alpha_detail * ( sum_{?rb: robot, ?s: situation} [
                if ( is_active_situation(?rb, ?h, ?s) ) then ( sum_{?rt: response_type} [ p_resp(?rt, ?s) * p_summary_detail_given_rt(?rt) ] ) else 0
            ] )
        );
        duration_short_wanted'(?h) = Bernoulli(
            (1 - alpha_duration) * short_duration_prob(?h)
          + alpha_duration * ( sum_{?rb: robot, ?s: situation} [
                if ( is_active_situation(?rb, ?h, ?s) ) then ( sum_{?rt: response_type} [ p_resp(?rt, ?s) * p_short_duration_given_rt(?rt) ] ) else 0
            ] )
        );
        justification_contrastive_wanted'(?h) = Bernoulli(
            (1 - alpha_justification) * justification_contrastive_prob(?h)
          + alpha_justification * ( sum_{?rb: robot, ?s: situation} [
                if ( is_active_situation(?rb, ?h, ?s) ) then ( sum_{?rt: response_type} [ p_resp(?rt, ?s) * p_just_contrastive_given_rt(?rt) ] ) else 0
            ] )
        );

        // --- Attribute selection persistence/update (mutual exclusivity within families)
        modality_visual_selected'(?r, ?h, ?b) =
            choose_visual_modality(?r, ?h, ?b) | (modality_visual_selected(?r, ?h, ?b) & ~choose_textual_modality(?r, ?h, ?b));
        modality_textual_selected'(?r, ?h, ?b) =
            choose_textual_modality(?r, ?h, ?b) | (modality_textual_selected(?r, ?h, ?b) & ~choose_visual_modality(?r, ?h, ?b));

        scope_local_selected'(?r, ?h, ?b) =
            choose_local_scope(?r, ?h, ?b) | (scope_local_selected(?r, ?h, ?b) & ~choose_global_scope(?r, ?h, ?b));
        scope_global_selected'(?r, ?h, ?b) =
            choose_global_scope(?r, ?h, ?b) | (scope_global_selected(?r, ?h, ?b) & ~choose_local_scope(?r, ?h, ?b));

        detail_summary_selected'(?r, ?h, ?b) =
            choose_summary_detail(?r, ?h, ?b) | (detail_summary_selected(?r, ?h, ?b) & ~choose_narrative_detail(?r, ?h, ?b));
        detail_narrative_selected'(?r, ?h, ?b) =
            choose_narrative_detail(?r, ?h, ?b) | (detail_narrative_selected(?r, ?h, ?b) & ~choose_summary_detail(?r, ?h, ?b));

        duration_short_selected'(?r, ?h, ?b) =
            choose_short_duration(?r, ?h, ?b) | (duration_short_selected(?r, ?h, ?b) & ~choose_long_duration(?r, ?h, ?b));
        duration_long_selected'(?r, ?h, ?b) =
            choose_long_duration(?r, ?h, ?b) | (duration_long_selected(?r, ?h, ?b) & ~choose_short_duration(?r, ?h, ?b));

        justification_standard_selected'(?r, ?h, ?b) =
            choose_justification_standard(?r, ?h, ?b) | (justification_standard_selected(?r, ?h, ?b) & ~choose_justification_contrastive(?r, ?h, ?b));
        justification_contrastive_selected'(?r, ?h, ?b) =
            choose_justification_contrastive(?r, ?h, ?b) | (justification_contrastive_selected(?r, ?h, ?b) & ~choose_justification_standard(?r, ?h, ?b));        

    }


    // ===== Action Preconditions (guards & exclusivity) =====
    action-preconditions {

        // --- Movement constraints
        // A robot must be in a position to move to another
        forall_{?r: robot, ?wf: waypoint, ?wt: waypoint} [goto_waypoint(?r, ?wf, ?wt) => ( robot_at(?r, ?wf) ^ connected(?wf, ?wt) ^ ?wf ~= ?wt ) ];
        // A robot can not be in two places at the same time
        forall_{?r: robot, ?w1: waypoint, ?w2: waypoint} [ ?w1 == ?w2 | (robot_at(?r, ?w1) => ~robot_at(?r, ?w2)) ];        

        // --- Fetch/give/ask_for book constraints
        forall_{?r: robot, ?bk: book, ?w: waypoint} [
            fetch_book(?r, ?bk) => ( robot_at(?r, ?w) ^ book_at(?bk, ?w) ^ ~holding(?r, ?bk) ^ 
                                    exists_{?s: situation} [ is_active_situation(?s) ^ situation_allows_fetching(?s) ] ^ 
                                    exists_{?h: human} [ asked_for_book(?h, ?r, ?bk) ] )
        ];
        forall_{?r: robot, ?bk: book, ?h: human} [
            give_book(?r, ?bk, ?h) => ( exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ 
                                        holding(?r, ?bk) ^ not_holding(?r, ?bk) ^ asked_for_book(?h, ?r, ?bk) )
        ];
        forall_{?h: human, ?r: robot, ?bk: book} [
            ask_for_book(?h, ?r, ?bk) => ( exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] )
        ];

        // --- Response selection (only for active situation; at-most-one per step)
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            respond(?rt, ?r, ?h, ?s) => ( 
                ( Bernoulli(p_resp(rt, s)) ^ is_active_situation(?r, ?h, ?s) ^ ~is_explanation_type(?rt) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ ~situation_allows_fetching(s) ) |
                ( Bernoulli(p_resp(rt, s)) ^ is_active_situation(?r, ?h, ?s) ^ ~is_explanation_type(?rt) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ situation_allows_fetching(s) ^ exists_{?bk: book} [ delivered(?bk, ?h) ] ) |
                ( is_active_situation(?r, ?h, ?s) ^ is_explanation_type(?rt) ^ explanation_planning_ended(?rt, ?r, ?h) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ ~situation_allows_fetching(s) ) | 
                ( is_active_situation(?r, ?h, ?s) ^ is_explanation_type(?rt) ^ explanation_planning_ended(?rt, ?r, ?h) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ situation_allows_fetching(s) ^ exists_{?bk: book} [ delivered(?bk, ?h) ] ) 
                ) ];
        forall_{?r: robot, ?h: human, ?s: situation, ?rt1: response_type, ?rt2: response_type} [
            (?rt1 ~= ?rt2) => ~( respond(?rt1, ?r, ?h, ?s) ^ respond(?rt2, ?r, ?h, ?s) )
        ];

        // --- Explanation planning sequence preconditions
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            start_explanation_planning(?rt, ?r, ?h, ?s) => ( 
                ( ~explanation_planning_started(?rt, ?r, ?h, ?s) ^ Bernoulli(p_resp(rt, s)) ^ is_explanation_type(?rt) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ ~situation_allows_fetching(s) ) |
                ( ~explanation_planning_started(?rt, ?r, ?h, ?s) ^ Bernoulli(p_resp(rt, s)) ^ is_explanation_type(?rt) ^ exists_{?w: waypoint} [ robot_at(?r, ?w) ^ human_at(?h, ?w) ] ^ situation_allows_fetching(s) ^ exists_{?bk: book} [ delivered(?bk, ?h) ] )
            ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            end_explanation_planning(?rt, ?r, ?h, ?s) =>
            ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_selected(?rt, ?r, ?h, ?s) | detail_narrative_selected(?rt, ?r, ?h, ?s))
              ^ (duration_short_selected(?rt, ?r, ?h, ?s) | duration_long_selected(?rt, ?r, ?h, ?s))
              ^ (justification_standard_selected(?rt, ?r, ?h, ?s) | justification_contrastive_selected(?rt, ?r, ?h, ?s)) 
            ) ];

        // --- Explanation gating: attribute choices only when an explanation-type response is active in the current situation
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_visual_modality(?rt, ?r, ?h, ?s) =>
                (explanation_planning_started(?rt, ?r, ?h, ?s) ^ modality_visual_wanted(?h))              
        ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_textual_modality(?rb, ?h, ?b) =>
                (explanation_planning_started(?rt, ?r, ?h, ?s) ^ modality_visual_wanted(?h))
        ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_local_scope(?rt, ?r, ?h, ?s) =>
                (explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_global_scope(?rt, ?r, ?h, ?s) =>
                (explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (~scope_local_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_summary_detail(?rt, ?r, ?h, ?s) =>
                (explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_narrative_detail(?rt, ?r, ?h, ?s) =>
                ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (~detail_summary_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_short_duration(?rt, ?r, ?h, ?s) =>
                ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_selected(?rt, ?r, ?h, ?s) | detail_narrative_selected(?rt, ?r, ?h, ?s))
              ^ (duration_short_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_long_duration(?rt, ?r, ?h, ?s) =>
                ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_selected(?rt, ?r, ?h, ?s) | detail_narrative_selected(?rt, ?r, ?h, ?s))
              ^ (~duration_short_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [
            choose_justification_standard(?rt, ?r, ?h, ?s) =>
                ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_selected(?rt, ?r, ?h, ?s) | detail_narrative_selected(?rt, ?r, ?h, ?s))
              ^ (duration_short_selected(?rt, ?r, ?h, ?s) | duration_long_selected(?rt, ?r, ?h, ?s))
              ^ (justification_standard_wanted(?rt, ?r, ?h, ?s)) 
        ) ];
        forall_{?rb: robot, ?h: human, ?b: book} [
            choose_justification_contrastive(?rt, ?r, ?h, ?s) =>
                ( explanation_planning_started(?rt, ?r, ?h, ?s)
              ^ (modality_visual_selected(?rt, ?r, ?h, ?s) | modality_textual_selected(?rt, ?r, ?h, ?s))
              ^ (scope_local_selected(?rt, ?r, ?h, ?s) | scope_global_selected(?rt, ?r, ?h, ?s))
              ^ (detail_summary_selected(?rt, ?r, ?h, ?s) | detail_narrative_selected(?rt, ?r, ?h, ?s))
              ^ (duration_short_selected(?rt, ?r, ?h, ?s) | duration_long_selected(?rt, ?r, ?h, ?s))
              ^ (~justification_standard_wanted(?rt, ?r, ?h, ?s))
        ) ];

        // --- Attribute mutual exclusivity per step
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [ ~( choose_visual_modality(?rt, ?r, ?h, ?s) ^ choose_textual_modality(?rt, ?r, ?h, ?s) ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [ ~( choose_local_scope(?rt, ?r, ?h, ?s) ^ choose_global_scope(?rt, ?r, ?h, ?s) ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [ ~( choose_summary_detail(?rt, ?r, ?h, ?s) ^ choose_narrative_detail(?rt, ?r, ?h, ?s) ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [ ~( choose_short_duration(?rt, ?r, ?h, ?s) ^ choose_long_duration(?rt, ?r, ?h, ?s) ) ];
        forall_{?rt: response_type, ?r: robot, ?h: human, ?s: situation} [ ~( choose_justification_standard(?rt, ?r, ?h, ?s) ^ choose_justification_contrastive(?rt, ?r, ?h, ?s) ) ];

    }

}
